## 机器学习面试题

**1.对机器学习概念的理解？**

机器学习是人工智能的一个子领域，通过让模型从数据中自动学习规律，并能够对没见过的数据进行预测或决策，即求解y=f(x)中的f，核心是用数据来**优化**模型性能。

**从任务的角度来说**，机器学习的两大基本任务，一个是分类，一个是回归，分类就是将输入的数据划分到不同的类别，而回归就是预测连续值。

**从学习范式的角度来说**，机器学习分为监督学习，无监督学习，半监督学习，自监督学习，强化学习等，监督学习就是用带标签的数据进行模型的训练，而无监督学习是让模型从没有标签的数据进行训练，比如聚类。

---------------------------------------------------------------------------

**2.监督学习、无监督学习、半监督学习、自监督学习是什么？有什么区别？**

**监督学习**是让模型从带标注的数据中进行学习，学习输入与输出的一个映射关系。

**无监督学习**是让模型从不带标注的数据中进行学习，目的是挖掘数据中潜在的结构和模式，比如说聚类，降维。

**半监督学习**介于监督学习和无监督学习之间，一般是结合少量的标注数据和大量的未标注数据进行训练，利用未标注数据的分布来辅助标注数据的学习，比如医学上的标注图像数据较少，可以采用这种方式。

**自监督学习**是通过设置前置任务来自动生成监督信号，从数据自身来生成伪标签来进行学习，比如现在预训练基底模型就是通过自监督学习的方式。

------------------------------

**3.什么是损失函数？有什么作用？**

损失函数(Loss Function)，也叫代价函数(Cost Function)，是机器学习中用来量化模型预测值与真实值差异的函数，用于指导模型参数的更新，使模型的预测更加准确，这样，我们就把模型训练的问题转化为了最小化损失函数的一个优化问题。

**扩展知识**——常见的损失函数

>  回归任务：

**均方误差(MSE、Mean Squared Error)**

特点：对离群点敏感，梯度大
$$
L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
**平均绝对误差(MAE、Mean Absolute Error)**

特点：对离群点鲁棒，梯度恒定
$$
L = \frac{1}{n} \sum_{i=1}^{n} \| y_i - \hat{y}_i \|
$$
**Huber Loss**

特点：平衡MSE和MAE的优点，L单指某点，最后要除以n
$$
L = \begin{cases} \frac{1}{2} (y_i - \hat{y}_i)^2 & \text{if } \| y_i - \hat{y}_i \| \leq \delta \\ \delta \| y_i - \hat{y}_i \| - \frac{1}{2} \delta^2 & \text{otherwise} \end{cases}
$$

> 分类任务

**交叉熵损失(Cross-Entropy)**

特点：分类任务的标准损失函数
$$
L = - \frac{1}{n} \sum_{i=1}^{n} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)
$$
**Hinge Loss(SVM)**

特点：支持向量机专用
$$
L = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \cdot \hat{y}_i)
$$
**Focal Loss**

特点：用于解决类别不平衡的问题，在难分类的类别中，Loss的值会较大，而易分类的类别中，Loss的值会较小
$$
FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t) \\
\alpha_t为类别平衡因子,\gamma为调节因子,最后相加除以n即可
$$
-----------------------------------------------

**4.什么是梯度下降？它的工作原理是什么？**

 梯度下降的核心思想是，沿着函数梯度的反方向逐步调整参数，使目标函数的值不断减小，具体工作原理如下：

1）初始化参数：随机选择一组初始参数值

2）计算梯度：计算当前位置的目标函数梯度，梯度反映了函数在该点的增长方向

3）参数更新：沿着梯度的反方向移动一步，即θ=θ-α*gradient

4）迭代过程：重复步骤2和3直到满足停止条件，比如变化小于某个阈值，或者迭代次数达到上限

梯度下降的变体：

1） 批量梯度下降(BGD、Batch Gradient Descent)：使用全部训练数据计算梯度

2） 随机梯度下降(SGD、Stochastic Gradient Descent)：每次使用一个样本计算梯度

3） 小批量梯度下降(Mini-batch Gradient Descent)：折中方案，每次使用一小批样本计算梯度

---------------------------------------

**5.什么是正则化？正则化的作用？常见的正则化方法？**

正则化（Regularization）是机器学习用于防止模型过拟合的一种手段，通过在模型训练过程中引入额外的约束或惩罚项，来限制模型的复杂度，使模型保持良好的泛化能力，防止过拟合。

**正则化的作用如下：**

1） 防止过拟合，限制模型复杂度，保证模型在训练集和测试集上表现良好

2） 提高泛化能力，帮助模型学习数据的普遍规律而非噪声

3） 特征选择，有些正则化方法可以自动进行特征选择，稀疏化模型参数，使模型更加简洁

**常见的正则化方法：**

1） L1正则化（Lasso回归）

- 在损失函数中添加权重参数的L1范数
- 会产生稀疏解，可用于特征选择

- 公式：L = L₀ + λ∑|wᵢ|

2） L2正则化（Ridge回归）

- 在损失函数中添加权重参数的L2范数平方
- 使权重参数趋向于较小值但不为零
- 公式：L = L₀ + λ∑wᵢ²

3） Dropout（神经网络特有）

- 训练时随机"丢弃"部分神经元
- 相当于训练多个子网络并平均

4） 早停（Early Stopping）

- 在验证集性能开始下降时停止训练
- 防止模型过度优化训练数据

**L1正则化和L2正则化的例子：**

假设我们有一个简单的线性回归模型：

y=w1\*x1+w2\*x2+b

其中：

- w1,w2是权重（系数）
- b是偏置项
- 损失函数（均方误差 MSE）为：

$$
L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

**训练后可能得到：**
w1=5,w2=3（假设这是最优解，但可能过拟合）

**L1 正则化**在损失函数中增加 **权重的绝对值之和**，公式：
$$
L_{\text{Lasso}} = \underbrace{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2}_{\text{MSE}} + \lambda \sum_{j=1}^{p} |w_j
$$
其中：

- λ是正则化强度（超参数）
- 惩罚项是 λ(∣w1∣+∣w2∣)

L1 的特点

✅ **倾向于让部分权重直接变为 0**（稀疏性）
✅ **适用于特征选择**（自动剔除不重要的特征）
❌ **对异常值敏感**（因为绝对值不可导）

### **例子**

假设 λ=1，优化后可能得到：

- w1=4,w2=0（L1 让 w2变成0，实现特征选择）

**L2 正则化**在损失函数中增加 **权重的平方和**，公式：
$$
L_{\text{Ridge}} = \underbrace{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2}_{\text{MSE}} + \lambda \sum_{j=1}^{p} w_j^2
$$
其中：

- 惩罚项是 λ(w1^2+w2^2)

### **L2 的特点**

✅ **让所有权重都变小，但不为 0**（平滑约束）
✅ **对异常值不敏感**（平方项使大权重惩罚更重）
❌ **不能做特征选择**（所有特征都会被保留）

### **例子**

假设 λ=1，优化后可能得到：

- w1=3.5,w2=1.8（L2 让权重整体缩小，但不归零）

-----------------------------

**6.什么是降维？为什么要降维？降维的优缺点？**

降维就是将高维数据转换为低维数据，同时尽量保留原始数据的信息，主要目的是减少计算的复杂度，减少过拟合的风险，有时也能帮助可视化数据，也能帮助数据存储。降维的方法常见的有主成分分析（PCA），线性判别分析（LDA），奇异值分解（SVD）。

**降维的优缺点：**

优点如下：

1） 减小计算复杂度

2） 降低过拟合风险

3） 帮助可视化

4） 减轻存储压力

缺点如下：

1） 信息损失

2） 选择合适的降维方法具有难度

3） 可解释性差

-----------------

**7.说明PCA算法的原理和步骤？**

PCA，全称Principal Component Analysis，主成分分析，基本原理是通过线性代数将数据转换到一个新的坐标系中，使得新坐标轴尽可能保留原始数据的方差，简单来说，PCA试图找到使得数据投影后方差最大的方向。同时，PCA属于线性降维方法，对于非线性关系的数据，效果可能不理想，这种情况下，可以选择非线性降维方法如t-SNE。

**PCA算法的步骤如下**：

1） 标准化数据，将数据转化为均值为0，方差为1，确保不同特征有相同的度量标准

2） 计算协方差矩阵，协方差矩阵反映了数据中各个特征质检的线性关系

3） 计算协方差矩阵的特征值和特征向量，特征向量代表了新的坐标轴方向，特征值代表了这些方向上的方差大小

4） 选择主成分，选择最大的几个特征值对应的特征向量，以此作为降维后的新特征

5） 生成新的数据集，将原始数据投影到选定的特征向量上，得到降维后的数据集













