## 机器学习面试题

**1.对机器学习概念的理解？**

机器学习是人工智能的一个子领域，通过让模型从数据中自动学习规律，并能够对没见过的数据进行预测或决策，即求解y=f(x)中的f，核心是用数据来**优化**模型性能。

**从任务的角度来说**，机器学习的两大基本任务，一个是分类，一个是回归，分类就是将输入的数据划分到不同的类别，而回归就是预测连续值。

**从学习范式的角度来说**，机器学习分为监督学习，无监督学习，半监督学习，自监督学习，强化学习等，监督学习就是用带标签的数据进行模型的训练，而无监督学习是让模型从没有标签的数据进行训练，比如聚类。

---------------------------------------------------------------------------

**2.监督学习、无监督学习、半监督学习、自监督学习是什么？有什么区别？**

**监督学习**是让模型从带标注的数据中进行学习，学习输入与输出的一个映射关系。

**无监督学习**是让模型从不带标注的数据中进行学习，目的是挖掘数据中潜在的结构和模式，比如说聚类，降维。

**半监督学习**介于监督学习和无监督学习之间，一般是结合少量的标注数据和大量的未标注数据进行训练，利用未标注数据的分布来辅助标注数据的学习，比如医学上的标注图像数据较少，可以采用这种方式。

**自监督学习**是通过设置前置任务来自动生成监督信号，从数据自身来生成伪标签来进行学习，比如现在预训练基底模型就是通过自监督学习的方式。

------------------------------

**3.什么是损失函数？有什么作用？**

损失函数(Loss Function)，也叫代价函数(Cost Function)，是机器学习中用来量化模型预测值与真实值差异的函数，用于指导模型参数的更新，使模型的预测更加准确，这样，我们就把模型训练的问题转化为了最小化损失函数的一个优化问题。

**扩展知识**——常见的损失函数

>  回归任务：

**均方误差(MSE、Mean Squared Error)**

特点：对离群点敏感，梯度大
$$
L = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$
**平均绝对误差(MAE、Mean Absolute Error)**

特点：对离群点鲁棒，梯度恒定
$$
L = \frac{1}{n} \sum_{i=1}^{n} \| y_i - \hat{y}_i \|
$$
**Huber Loss**

特点：平衡MSE和MAE的优点，L单指某点，最后要除以n
$$
L = \begin{cases} \frac{1}{2} (y_i - \hat{y}_i)^2 & \text{if } \| y_i - \hat{y}_i \| \leq \delta \\ \delta \| y_i - \hat{y}_i \| - \frac{1}{2} \delta^2 & \text{otherwise} \end{cases}
$$

> 分类任务

**交叉熵损失(Cross-Entropy)**

特点：分类任务的标准损失函数
$$
L = - \frac{1}{n} \sum_{i=1}^{n} y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)
$$
**Hinge Loss(SVM)**

特点：支持向量机专用
$$
L = \frac{1}{n} \sum_{i=1}^{n} \max(0, 1 - y_i \cdot \hat{y}_i)
$$
**Focal Loss**

特点：用于解决类别不平衡的问题，在难分类的类别中，Loss的值会较大，而易分类的类别中，Loss的值会较小
$$
FL(p_t) = -\alpha_t (1 - p_t)^\gamma \log(p_t) \\
\alpha_t为类别平衡因子,\gamma为调节因子,最后相加除以n即可
$$
**4.什么是梯度下降？它的工作原理是什么？**

 梯度下降的核心思想是，沿着函数梯度的反方向逐步调整参数，使目标函数的值不断减小，具体工作原理如下：

1）初始化参数：随机选择一组初始参数值

2）计算梯度：计算当前位置的目标函数梯度，梯度反映了函数在该点的增长方向

3）参数更新：沿着梯度的反方向移动一步，即θ=θ-α*gradient

4）迭代过程：重复步骤2和3直到满足停止条件，比如变化小于某个阈值，或者迭代次数达到上限

梯度下降的变体：

1） 批量梯度下降(BGD、Batch Gradient Descent)：使用全部训练数据计算梯度

2） 随机梯度下降(SGD、Stochastic Gradient Descent)：每次使用一个样本计算梯度

3） 小批量梯度下降(Mini-batch Gradient Descent)：折中方案，每次使用一小批样本计算梯度

**5.什么是正则化？正则化的作用？常见的正则化方法？**